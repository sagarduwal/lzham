# Introduction #

Graham Fyffe published this article on a near-optimal binary prefix code construction method on his Geocities website around 10 years ago, but unfortunately it's no longer available. I've placed it here for historical purposes, because (as far as I know anyway) it's one of the few alternatives to Shannon-Fano coding, and was developed before Polar's method.

I implemented Fyffe codes in an experimental form in LZHAM last year, but my initial implementation wasn't any faster than linear time Huffman. My first implementation had some deficiencies however, so I may try again.

This article was cited by this paper:
[Fast Construction of Disposable Prefix-Free Codes](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.125.4017), by Danny Dubé, Université Laval

## Fyffe Codes for Fast Codelength Approximation ##
By Graham Fyffe - 1999

Here I present a fast algorithm for generating near-optimal binary prefix codes. I call them "Fyffe codes" for now, because to my knowledge they are a new kind of code. I discovered these codes in July 1997, and have recently decided to share them with the planet. If someone has previous knowledge of the existence of such codes, please contact me!

The goal here is to quickly approximate the codelengths generated by the Huffman algorithm. We can note that a Huffman code will never be as far as 1 bit from the optimal length, with exception for the last code which may be shorter. This means that the codelength is either floor(-log2(p(x))) or cieling(-log2(p(x))). It seems reasonable to initially set each codelength to cieling(-log2(p(x))), and then promote some of them to floor(-log2(p(x))) until we have a full prefix set. This condition can be determined as follows:

Define the residual R to be 1 - the sum over x of 2^-codelength(x). It should be clear that we have a full prefix set iff R = 0.

Now we must decide which strings to promote. We can restrict the set of candidate strings as follows:

Define U(x) as the unhappiness of a string x as follows:

```
U(x) = (codelength(x)+log2(p(x))) * p(x)
```

A string x is said to be unhappy if U(x) > 0. Otherwise x is said to be happy. Unhappy strings are those whose codes are longer than optimal; happy strings are those whose codes are not. It should be fairly evident that only unhappy strings should be considered for promotion.

It should be noted that once a string x is promoted, it becomes happy because U(x) will no longer be greater than zero. It should also be noted that there are cases where all strings are happy (after some promotions), and yet R > 0. In these cases, R will be just big enough to allow one more promotion of a string with the longest code, and the most probable such string should thusly be promoted.

A further restriction on the set of candidates is as follows:

Note that promoting a string will always decrease its codelength by one bit, because only unhappy strings are considered, and therefore have codelengths that are greater than optimal, but always by less than one unit. Thus, promoting a string x decreases the residual R by 2^-codelength(x). This is easy to verify:

```
R - R'
= K-2^-codelength(x) - (K-2^-(codelength(x)-1))
= K - 2^-codelength(x) - K + 2^(codelength(x)+1)
= -2^-codelength(x) + 2^-(codelength(x)) * 2
= 2^-codelength(x)
```

So for any string x, if 2^-codelength(x) > R, promotion is said to be illegal since R would drop below zero.

Thus we can restrict the set of strings to consider for promotion to those that are unhappy and will not cause R to drop below 0.

We now have sufficiently reduced the set of possible promotions to design a naive algorithm for generating a reasonable prefix code set: simply promote the most unhappy string that can be legally promoted until R = 0, thus yielding a full-prefix set. This algorithm makes the assumption that, given a set of legally promotible strings, it is always most desirable to promote the most unhappy string. This can easily be proven sub-optimal by counter-example, but actually turns out to exactly duplicate Huffman codelengths in most cases, and can be implemented to perform quite quickly.

There is an optimization to be noted here. If we are given the strings sorted by probability, and scan the strings from greatest to least probability, then the most unhappy string that can be legally promoted will be the first legally promotable unhappy string we encounter. If the strings are not already sorted, then they would have to be sorted before proceeding.

I shall illustrate the process with a simple example:

Say we have a set of strings A, B, C, D with respecitve probabilities 0.6, 0.25, 0.1, 0.05. For each string x, compute C(x) = cieling(-log2(p(x))), U(x) and R:
```
C(x) = {1, 2, 4, 5}
U(x) = {0.1578, 0.0000, 0.0678, 0.0339}
R = 0.15625
```

That's the initialization step. Note that we can compute cieling(-log2(p(x))) with bitwise machine instructions, and compute R with fixed-point arithmetic, provided we chose the precision of p(x) a priori. U(x) is not actually computed.

Now we traverse the list in order of greatest to least unhappiness. This is simply from left to right.

The first string we inspect is A. Using bitwise machine instructions, we find that 2^-codelength(A) > R, thus we do not perform this promotion.

The next string we inspect is B. B is happy, so we do nothing.

The next string we inspect is C. We find that 2^-codelength(C) =< R, thus we perform the promotion: C's codelength becomes 3, and R becomes 0.09375.

The next string we inspect is D. We find that 2^-codelength(D) =< R, thus we perform the promotion: D's codelength becomes 4, and R becomes 0.03125.

We have now traversed all the strings. All that is left to do is test if R > 0, and if so, promote the last string, in this case D. We find that indeed R > 0, so D's codelength becomes 3.

The final codelengths are C(x) = {1, 2, 3, 3}, from which we can generate the traditional canonical codeset {0, 10, 110, 111}.

This algorithm always finishes with just one pass through the strings, and thus has algorithmic complexity O(n), provided that the strings are already sorted in order of probability. An initial implementation required 350 CPU cycles per code to compute the code table on a Pentium II. This could be a little smaller with optimizations.

Further points to investigate include an analysis of the error incurred by this approximate method, thus determining the loss in efficiency.

- Graham Fyffe ©1999